# vllm.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token
type: Opaque
stringData:
  HF_TOKEN: "YOUR_HF_TOKEN_IF_NEEDED"  # remove if using open models

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-infer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-infer
  template:
    metadata:
      labels:
        app: vllm-infer
    spec:
      containers:
        - name: api
          image: saujandsre/vllm-gpu:min
          imagePullPolicy: IfNotPresent
          env:
            - name: MODEL_PATH
              value: "Qwen/Qwen2.5-3B-Instruct"   # swap to any model ID
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: HF_TOKEN
          ports:
            - containerPort: 8000
          resources:
            requests:
              nvidia.com/gpu: 1
            limits:
              nvidia.com/gpu: 1
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-svc
spec:
  selector:
    app: vllm-infer
  ports:
    - name: http
      port: 8000
      targetPort: 8000
  type: ClusterIP

